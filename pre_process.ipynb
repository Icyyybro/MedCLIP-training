{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b099c5fa-1051-4516-84a8-7f26f8f5cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a62b79-b609-4ba5-93f5-c8c69ad2069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始标注文件\n",
    "ann_path = \"/root/autodl-tmp/mimic_cxr/mimic_annotation_promptmrg.json\"\n",
    "# 原始图像目录\n",
    "images_dir = \"/root/autodl-tmp/mimic_cxr/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b0e3c8-51dd-4565-a642-7e0fc3fdb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本清理\n",
    "def clean_report_mimic_cxr(report):\n",
    "    report_cleaner = lambda t: t.replace('\\n', ' ').replace('__', '_').replace('__', '_').replace('__', '_') \\\n",
    "            .replace('__', '_').replace('__', '_').replace('__', '_').replace('__', '_').replace('  ', ' ') \\\n",
    "            .replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ') \\\n",
    "            .replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.') \\\n",
    "            .replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '').replace('. 2. ', '. ') \\\n",
    "            .replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ').replace(' 2. ', '. ') \\\n",
    "            .replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n",
    "            .strip().lower().split('. ')\n",
    "    sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip().lower())\n",
    "    tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != []]\n",
    "    report = ' . '.join(tokens) + ' .'\n",
    "    return report\n",
    "\n",
    "def my_pre_caption(caption, max_words=100):\n",
    "    caption = clean_report_mimic_cxr(caption)\n",
    "    #truncate caption\n",
    "    caption_words = caption.split(' ')\n",
    "    if len(caption_words)>max_words:\n",
    "        caption = ' '.join(caption_words[:max_words])\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0761fd9d-4f3c-4746-9cfb-51ffbfe80d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字标签与对应的文本描述\n",
    "SCORES = [\n",
    "'[BLA]',\n",
    "'[POS]',\n",
    "'[NEG]',\n",
    "'[UNC]'\n",
    "]\n",
    "\n",
    "class generation_train(Dataset):\n",
    "    def __init__(self, transform, image_root, ann_root, max_words=100):\n",
    "        self.annotation = json.load(open(os.path.join(ann_root),'r'))\n",
    "        self.transform = transform\n",
    "        self.image_root = image_root\n",
    "        self.max_words = max_words\n",
    "        self.all_ann = self.annotation['train']\n",
    "        self.ann = []  # 子集标注样本\n",
    "        \n",
    "        for idx, ann in enumerate(self.all_ann):\n",
    "            image_path = ann['image_path']\n",
    "            full_path = os.path.join(self.image_root, image_path[0])\n",
    "            if os.path.exists(full_path):\n",
    "                # 图片存在，添加到ann中\n",
    "                self.ann.append(ann)\n",
    "            else:\n",
    "                # 遇到第一个不存在的图片就停止\n",
    "                break\n",
    "                \n",
    "        with open('/root/autodl-tmp/mimic_cxr/clip_text_features.json', 'r') as f:\n",
    "            self.clip_features = np.array(json.load(f))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ann)\n",
    "    \n",
    "    def __getitem__(self, index):    \n",
    "        \n",
    "        ann = self.ann[index]\n",
    "        image_path = ann['image_path']\n",
    "        image = Image.open(os.path.join(self.image_root, image_path[0])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        cls_labels = ann['labels']\n",
    "        prompt = [SCORES[l] for l in cls_labels]\n",
    "        prompt = ' '.join(prompt)+' '\n",
    "        caption = prompt + my_pre_caption(ann['report'], self.max_words)\n",
    "        cls_labels = torch.from_numpy(np.array(cls_labels)).long()\n",
    "        clip_indices = ann['clip_indices'][:21]\n",
    "        clip_memory = self.clip_features[clip_indices]\n",
    "        clip_memory = torch.from_numpy(clip_memory).float()\n",
    "\n",
    "        return image, caption, cls_labels, clip_memory\n",
    "    \n",
    "class generation_eval(Dataset):\n",
    "    def __init__(self, transform, image_root, ann_root, max_words=100, split='val'):\n",
    "        self.annotation = json.load(open(os.path.join(ann_root), 'r'))\n",
    "        self.transform = transform\n",
    "        self.image_root = image_root\n",
    "        self.max_words = max_words\n",
    "        self.all_ann = self.annotation[split]\n",
    "        self.ann = []  # 子集标注样本\n",
    "        \n",
    "        for idx, ann in enumerate(self.all_ann):\n",
    "            image_path = ann['image_path']\n",
    "            full_path = os.path.join(self.image_root, image_path[0])\n",
    "            if os.path.exists(full_path):\n",
    "                # 图片存在，添加到ann中\n",
    "                self.ann.append(ann)\n",
    "            else:\n",
    "                # 遇到第一个不存在的图片就停止\n",
    "                break\n",
    "            \n",
    "        \n",
    "        with open('/root/autodl-tmp/mimic_cxr/clip_text_features.json', 'r') as f:\n",
    "            self.clip_features = np.array(json.load(f))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ann)\n",
    "    \n",
    "    def __getitem__(self, index):    \n",
    "        \n",
    "        ann = self.ann[index]\n",
    "        image_path = ann['image_path']\n",
    "        image = Image.open(os.path.join(self.image_root, image_path[0])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        caption = my_pre_caption(ann['report'], self.max_words)\n",
    "        cls_labels = ann['labels']\n",
    "        cls_labels = torch.from_numpy(np.array(cls_labels))\n",
    "        clip_indices = ann['clip_indices'][:21]\n",
    "        clip_memory = self.clip_features[clip_indices]\n",
    "        clip_memory = torch.from_numpy(clip_memory).float()\n",
    "\n",
    "        return image, caption, cls_labels, clip_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac77608-c523-4361-aa60-45d9d1d9955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化并生成训练/验证/测试集\n",
    "def create_dataset(image_dir, ann_path):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((336, 336)),\n",
    "        transforms.RandomRotation(degrees=5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((336, 336)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    train_dataset = generation_train(transform_train, image_dir, ann_path)\n",
    "    val_dataset = generation_eval(transform_test, image_dir, ann_path, split='val')\n",
    "    test_dataset = generation_eval(transform_test, image_dir, ann_path, split='test')\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ce2bcb-61d0-4dcd-9a36-1d7391e47c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = create_dataset(images_dir, ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fde47084-b7b0-49ef-9760-5765a9cda6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "def save_datasets(train_dataset, val_dataset, test_dataset, save_path=\"/root/autodl-tmp/mimic_cxr/datasets.pkl\"):\n",
    "    datasets = {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset,\n",
    "        'info': {\n",
    "            'train_size': len(train_dataset),\n",
    "            'val_size': len(val_dataset),\n",
    "            'test_size': len(test_dataset),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(datasets, f)\n",
    "    \n",
    "    print(f\"数据集已保存到: {save_path}\")\n",
    "\n",
    "# 读取\n",
    "def load_datasets(save_path=\"/root/autodl-tmp/mimic_cxr/datasets.pkl\"):\n",
    "    with open(save_path, 'rb') as f:\n",
    "        datasets = pickle.load(f)\n",
    "    \n",
    "    train_dataset = datasets['train']\n",
    "    val_dataset = datasets['val']\n",
    "    test_dataset = datasets['test']\n",
    "    \n",
    "    print(\"数据集加载成功!\")\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6b88a9-0dfe-41af-a8ab-fea1723b32ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集已保存到: /root/autodl-tmp/mimic_cxr/datasets.pkl\n"
     ]
    }
   ],
   "source": [
    "save_datasets(train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb91ded-eed6-4ea0-a95a-9ded78082e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 26488\n",
      "测试集大小: 255\n",
      "验证集大小: 185\n"
     ]
    }
   ],
   "source": [
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")\n",
    "print(f\"验证集大小: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae1e128-90e4-45f6-8262-1e6bcac961e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理 0 条样本...\n",
      "已处理 10 条样本...\n",
      "已处理 20 条样本...\n",
      "已处理 30 条样本...\n",
      "已处理 40 条样本...\n",
      "已处理 50 条样本...\n",
      "已处理 60 条样本...\n",
      "已处理 70 条样本...\n",
      "已处理 80 条样本...\n",
      "已处理 90 条样本...\n",
      "样本已保存到 samples.txt\n"
     ]
    }
   ],
   "source": [
    "# 获取100条样本并保存到文件\n",
    "with open('samples.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(100):\n",
    "        image, caption, cls_labels, clip_memory = train_dataset[i]\n",
    "            \n",
    "        f.write(f\"样本 {i}:\\n\")\n",
    "        f.write(f\"  图像形状: {image.shape}\\n\")\n",
    "        f.write(f\"  文本: {caption}\\n\")\n",
    "        f.write(f\"  分类标签: {cls_labels}\\n\")\n",
    "        f.write(f\"  CLIP记忆形状: {clip_memory.shape}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")  # 分隔线\n",
    "            \n",
    "        if i % 10 == 0:\n",
    "            print(f\"已处理 {i} 条样本...\")\n",
    "print(\"样本已保存到 samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94f3d5a-d164-4764-b4bb-98adabe20749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集加载成功!\n"
     ]
    }
   ],
   "source": [
    "t1,t2,t3 = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f47324f-932a-4fd1-bf5f-e7a58f2f9a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本 0:\n",
      "  图像形状: torch.Size([3, 336, 336])\n",
      "  文本: no evidence of consolidation to suggest pneumonia is seen . there is some retrocardiac atelectasis . a small left pleural effusion may be present . no pneumothorax is seen . no pulmonary edema . a right granuloma is unchanged . the heart is mildly enlarged unchanged . there is tortuosity of the aorta .\n",
      "  分类标签: tensor([0, 1, 0, 0, 2, 2, 2, 1, 2, 1, 0, 0, 0, 0])\n",
      "  CLIP记忆形状: torch.Size([21, 512])\n"
     ]
    }
   ],
   "source": [
    "image, caption, cls_labels, clip_memory = t2[0]\n",
    "print(f\"样本 {0}:\")\n",
    "print(f\"  图像形状: {image.shape}\")\n",
    "print(f\"  文本: {caption}\")\n",
    "print(f\"  分类标签: {cls_labels}\")\n",
    "print(f\"  CLIP记忆形状: {clip_memory.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627a46e-3139-4634-8b4c-20f73fe70a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
